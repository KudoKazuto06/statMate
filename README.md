# ğŸ§® StatMate â€“ AI Statistics Assistant

**StatMate** is an intelligent statistics chatbot and computation tool built with **Streamlit**, **LangChain**, and **Ollama**.  
It allows users to upload data, chat about statistical concepts, and automatically perform the appropriate test â€” all locally, with **no API keys or cloud dependencies**.

---

## ğŸš€ Features

- ğŸ’¬ **AI Chatbot** â€“ Ask theoretical questions like *â€œWhat is random sampling?â€* or *â€œWhen to use ANOVA?â€*  
- ğŸ“Š **Automatic Test Selection** â€“ Detects and performs statistical tests such as:
  - Independent t-test  
  - Paired t-test  
  - ANOVA  
  - Correlation  
  - Descriptive statistics  
- ğŸ“‚ **Upload CSV/Excel** â€“ Analyze your own datasets directly.  
- ğŸ§  **Local LLM Integration** â€“ Powered by **Llama 3** running via **Ollama**.  
- ğŸ”’ **Private & Offline** â€“ No internet or external API calls required.

---

## ğŸ§© Tech Stack (Backbone)

| Layer | Technology |
|-------|-------------|
| **Frontend / UI** | [Streamlit](https://streamlit.io) |
| **AI Layer** | [LangChain](https://www.langchain.com) + [Ollama](https://ollama.com) |
| **Statistical Engine** | NumPy, SciPy, Pandas |
| **Model** | `llama3:8b` (customizable to smaller models like `llama3:latest`) |

---

## âš™ï¸ Installation & Run Locally

### 1ï¸âƒ£ Prerequisites
- [Python 3.9+](https://www.python.org/downloads/)
- [Ollama](https://ollama.com) installed on your machine
- Llama 3 model pulled locally:
  ```bash
  ollama pull llama3:8b
  ```

### 2ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<your-username>/statmate.git
cd statmate
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run the app
```bash
streamlit run main.py
```

The app will automatically start the Ollama server if itâ€™s not already running.

---

## ğŸ§  How It Works

1. StatMate ensures Ollama is active locally (`ollama serve`).  
2. When you enter a question:
   - If itâ€™s theoretical â†’ StatMate uses **Llama 3** to explain it.
   - If a dataset is uploaded â†’ it decides which test to apply and runs the computation.
3. The result (statistics or explanation) is displayed instantly in Streamlit.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ app.py               # Auto-starts Ollama server
â”œâ”€â”€ llm_utils.py         # AI model and logic (LLM wrappers)
â”œâ”€â”€ main.py              # Streamlit interface (chat + computation)
â”œâ”€â”€ stats_engine.py      # Statistical test implementations
â”œâ”€â”€ requirements.txt     # Dependencies
â””â”€â”€ README.md            # Project documentation
```

---

## âš¡ Example Questions

- â€œWhat is random sampling?â€  
- â€œCompare pre-test and post-test scores.â€  
- â€œIs there a correlation between height and weight?â€  
- â€œShow descriptive stats for my dataset.â€

---

## ğŸ§© Customization

Change the model in `llm_utils.py`:
```python
llm = OllamaLLM(model="llama3:latest")  # or "llama3:1b" for faster performance
```

---

## ğŸ“œ License & Copyright

```
Â© 2025 Nguyá»…n CÃ´ng Thá»‹nh.  
All rights reserved.

This project is for educational and personal research use only.
You may fork or modify it with proper attribution.
```

---

## ğŸ’¡ Acknowledgements

- [Streamlit](https://streamlit.io) for interactive UI.  
- [LangChain](https://www.langchain.com) for LLM orchestration.  
- [Ollama](https://ollama.com) for local LLM deployment.  
- [Llama 3](https://llama.meta.com/) by Meta for the underlying model.

---

**StatMate â€“ Chat. Analyze. Understand.**
